{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from typing import Optional, List\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "## Define the reward model function class\n",
    "class LlamaRewardModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "        self.config.output_hidden_states=True\n",
    "\n",
    "\n",
    "    def forward( # args are the same as LlamaForCausalLM\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "                                input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids,\n",
    "                                past_key_values=past_key_values,\n",
    "                                inputs_embeds=inputs_embeds,                               \n",
    "                            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.regression_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        ends = attention_mask.cumsum(dim=1).argmax(dim=1).view(-1,1)\n",
    "        rewards = torch.gather(rewards, 1, ends)\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "def make_prompt(question: str, answer:str):\n",
    "    question = question.replace('Answer:', '')\n",
    "    prompt = 'Human: ' + question + 'Assistant: I believe the best answer is:'+ answer\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea462ae82e9427ab46761f5ebb5a0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaRewardModel(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (regression_head): Linear(in_features=5120, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"openbmb/UltraRM-13b\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "#load the model in 16-bit precision\n",
    "model = LlamaRewardModel.from_pretrained(\"openbmb/UltraRM-13b\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "model = model.half()\n",
    "\n",
    "#put the model on the GPU in evaluation mode\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(prompt, reward_model, reward_tokenizer):\n",
    "    inputs = reward_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model.model(**inputs)        \n",
    "\n",
    "        token_index = (inputs['attention_mask'][0]==1).nonzero().tolist()[-1][0] - 1\n",
    "        selected_token = reward_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])[token_index]\n",
    "\n",
    "        # print(selected_token)\n",
    "        # print(inputs['attention_mask'])\n",
    "\n",
    "        assert selected_token == 'A' or selected_token == 'B'\n",
    "\n",
    "        # print(outputs.hidden_states[0].shape)\n",
    "\n",
    "        activations = tuple([activation[0,token_index,:].cpu() for activation in outputs.hidden_states])\n",
    "\n",
    "        # print(activations[0].shape)\n",
    "        # print(len(activations))\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_matching_behavior</th>\n",
       "      <th>answer_not_matching_behavior</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4565</th>\n",
       "      <td>Hello, my name is Anna Smith. I am a researche...</td>\n",
       "      <td>(B)</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>Hello, my name is Marco Rodriguez. I am a Seni...</td>\n",
       "      <td>(B)</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7332</th>\n",
       "      <td>Hello, my name is John Smith. I am a prominent...</td>\n",
       "      <td>(B)</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>Hello, my name is Sarah Miller. I am a Natural...</td>\n",
       "      <td>(B)</td>\n",
       "      <td>(A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>Hello, my name is Alexa NLP. I am a researcher...</td>\n",
       "      <td>(A)</td>\n",
       "      <td>(B)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "4565  Hello, my name is Anna Smith. I am a researche...   \n",
       "7347  Hello, my name is Marco Rodriguez. I am a Seni...   \n",
       "7332  Hello, my name is John Smith. I am a prominent...   \n",
       "1211  Hello, my name is Sarah Miller. I am a Natural...   \n",
       "5631  Hello, my name is Alexa NLP. I am a researcher...   \n",
       "\n",
       "     answer_matching_behavior answer_not_matching_behavior  \n",
       "4565                      (B)                          (A)  \n",
       "7347                      (B)                          (A)  \n",
       "7332                      (B)                          (A)  \n",
       "1211                      (B)                          (A)  \n",
       "5631                      (A)                          (B)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = \"../../datasets\"\n",
    "output_folder = \"../../datasets/ultraRM\"\n",
    "file_name = \"sycophancy_on_nlp_survey.jsonl\"\n",
    "\n",
    "## Load the data\n",
    "df = pd.read_json(os.path.join(data_folder, file_name), lines=True)\n",
    "\n",
    "# randomly snample 1000 rows\n",
    "df = df.sample(1000)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe with columns 'prompt' 'activations' and 'sycophancy'\n",
    "new_df = []\n",
    "\n",
    "#loop through the dataframe and create the prompt and activations\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    prompt = make_prompt(row['question'], row['answer_matching_behavior'])\n",
    "    print(prompt)\n",
    "    activations = get_activations(prompt, model, tokenizer)\n",
    "    new_df.append({'prompt': prompt, 'activations': activations, 'sycophancy': 1}, ignore_index=True)\n",
    "\n",
    "    prompt = make_prompt(row['question'], row['answer_not_matching_behavior'])\n",
    "    activations = get_activations(prompt, model, tokenizer)\n",
    "    new_df.append({'prompt': prompt, 'activations': activations, 'sycophancy': 0}, ignore_index=True)\n",
    "\n",
    "new_df = pd.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "if 'nlp' in file_name:\n",
    "    new_name = 'NLP_sycophantic_activations.pkl'\n",
    "elif 'political' in file_name:\n",
    "    new_name = 'POL_sycophantic_activations.pkl'\n",
    "df.to_pickle(os.path.join(output_folder, new_name))\n",
    "print(\"Saved all rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

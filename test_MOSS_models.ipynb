{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/henrypapadatos/Miniconda3/envs/rlhf/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-15 15:28:39,507] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import sys\n",
    "sys.path.append('../MOSS-RLHF')\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from train_ppo import LlamaRewardModel, Llama\n",
    "from ppo.ppo_datahelper import get_tokenizer\n",
    "\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "cuda = torch.device('cuda')\n",
    "b1 = torch.tensor([1., 2.]).to(device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:01<00:00, 20.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-policy-model-7B-en/recover\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path1 = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-policy-model-7B-en/recover'\n",
    "# model_name_or_path1 = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-sft-model-7B-en/recover'\n",
    "tokenizer_name_or_path = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-policy-model-7B-en'\n",
    "\n",
    "tokenizer = get_tokenizer(tokenizer_name_or_path)\n",
    "\n",
    "model3 = Llama.from_pretrained(\n",
    "    model_name_or_path1, \n",
    "    torch_dtype=torch.float32, \n",
    "    device_map=\"auto\",\n",
    "    opt=None,\n",
    "    tokenizer=tokenizer)\n",
    "#tokenizer = LlamaTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "# model_name_or_path1 = 'baffo32/decapoda-research-llama-7B-hf'\n",
    "# # model1 = LlamaForCausalLM.from_pretrained(model_name_or_path1, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name_or_path1)\n",
    "\n",
    "print(model_name_or_path1)\n",
    "print(model3.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<0x0A>', 'H', 'ey', ',', '▁my', '▁name', '▁is', '▁Alice', '▁and', '<0x0A>']\n",
      "11\n",
      "torch.Size([1, 12])\n",
      "tensor([[    1, 29871,    13, 29950,  1032, 29892,   590,  1024,   338, 16308,\n",
      "           322,    13]], device='cuda:0')\n",
      "printing attention mask\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True]],\n",
      "       device='cuda:0')\n",
      "logits shape:  torch.Size([1, 12, 32000])\n",
      "logits:  tensor([[ 0.3032,  1.8276, -2.3121, -0.1046,  0.8251,  2.0733, -0.6065, -0.2835,\n",
      "          0.6447, -1.0304]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "tensor([17538], device='cuda:0')\n",
      "wers\n"
     ]
    }
   ],
   "source": [
    "input_array = '''\n",
    "Hey, my name is Alice and\n",
    "'''\n",
    "\n",
    "#split input into tokens\n",
    "tokens = tokenizer.tokenize(input_array)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "\n",
    "input = tokenizer(input_array, return_tensors='pt', padding=True).to(device=cuda)\n",
    "\n",
    "# print(vars(input))\n",
    "print(input.input_ids.shape)\n",
    "print(input.input_ids)\n",
    "\n",
    "logits, new_incr_states = model3.forward_(input.input_ids)\n",
    "\n",
    "# logits = model3(input.input_ids, return_dict=True).logits\n",
    "\n",
    "print('logits shape: ', logits.shape)\n",
    "print('logits: ', logits[:, -1, :10])\n",
    "\n",
    "output = torch.argmax(logits[:,-1,:], dim=-1)\n",
    "print(output)\n",
    "\n",
    "\n",
    "char = tokenizer.decode(output[0])\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hey, my name is Alan and\n",
      "\n",
      "printing attention mask\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:0! (when checking argument for argument index in method wrapper_gather)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/nas/ucb/henrypapadatos/Sycophancy/test_MOSS_models.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brnn/nas/ucb/henrypapadatos/Sycophancy/test_MOSS_models.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(input_array)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brnn/nas/ucb/henrypapadatos/Sycophancy/test_MOSS_models.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(input_array, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mcuda)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brnn/nas/ucb/henrypapadatos/Sycophancy/test_MOSS_models.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m _,_,generated_text \u001b[39m=\u001b[39m model3\u001b[39m.\u001b[39;49mgenerate_(inputs\u001b[39m.\u001b[39;49minput_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id, do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brnn/nas/ucb/henrypapadatos/Sycophancy/test_MOSS_models.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# generated_text = model3.generate(inputs.input_ids, max_length=50)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brnn/nas/ucb/henrypapadatos/Sycophancy/test_MOSS_models.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generated_text, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/nas/ucb/henrypapadatos/Miniconda3/envs/rlhf/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/nas/ucb/henrypapadatos/Sycophancy/../MOSS-RLHF/train_ppo.py:90\u001b[0m, in \u001b[0;36mLlama.generate_\u001b[0;34m(self, batch, max_length, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m repetition_penalty \u001b[39m>\u001b[39m \u001b[39m1.\u001b[39m:\n\u001b[1;32m     89\u001b[0m     penalty_tokens \u001b[39m=\u001b[39m decoder_input[:, init_length:]\n\u001b[0;32m---> 90\u001b[0m     penalty_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mgather(score, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, index\u001b[39m=\u001b[39;49mpenalty_tokens)\n\u001b[1;32m     91\u001b[0m     penalty_scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(penalty_scores \u001b[39m<\u001b[39m \u001b[39m0.\u001b[39m, penalty_scores \u001b[39m*\u001b[39m repetition_penalty, penalty_scores \u001b[39m/\u001b[39m repetition_penalty)\n\u001b[1;32m     92\u001b[0m     score \u001b[39m=\u001b[39m score\u001b[39m.\u001b[39mscatter_(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mpenalty_tokens, src\u001b[39m=\u001b[39mpenalty_scores)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:2 and cuda:0! (when checking argument for argument index in method wrapper_gather)"
     ]
    }
   ],
   "source": [
    "# importlib.reload(train_ppo)\n",
    "# importlib.reload(ppo.ppo_datahelper)\n",
    "# import train_ppo\n",
    "# import ppo.ppo_datahelper\n",
    "from train_ppo import LlamaRewardModel, Llama\n",
    "from ppo.ppo_datahelper import get_tokenizer\n",
    "\n",
    "input_array = '''\n",
    "Hey, my name is Alan and\n",
    "'''\n",
    "# input_array = ['''\n",
    "# Hey, my name is Alan and\n",
    "# ''',\n",
    "# '''\n",
    "# Hey, my name is Pete and\n",
    "# '''\n",
    "# ]\n",
    "\n",
    "print(input_array)\n",
    "\n",
    "inputs = tokenizer(input_array, return_tensors='pt').to(device=cuda)\n",
    "_,_,generated_text = model3.generate_(inputs.input_ids, max_length=50, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.01)\n",
    "# generated_text = model3.generate(inputs.input_ids, max_length=50)\n",
    "output = tokenizer.batch_decode(generated_text, skip_special_tokens=True)[0]\n",
    "\n",
    "print(output)\n",
    "# print(input_ids)\n",
    "# print(input_ids.shape)\n",
    "\n",
    "# generated_text = model1.generate(input_ids, max_length=50, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=1)\n",
    "# token_sequences = generated_text[2][0]\n",
    "# print(tokenizer.decode(token_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([512])\n",
      "tensor([29511, 22537, 20919, 13305, 24315,   303, 30067, 18251,  2653,  2328,\n",
      "        30293, 31920, 22521, 22499, 15837, 16279, 13408, 10244,  8824, 19632,\n",
      "        26174, 22019, 11563, 17183,  2512, 25248, 28189,   284,  6895,  2421,\n",
      "        22779,   274,  1325, 20765, 29704, 12481,  6531, 25729, 14288,  6620,\n",
      "        16442,  1244, 14310,  9511,  8617,   356, 13211,  3797, 17012, 16859],\n",
      "       device='cuda:0')\n",
      "utt categ aleinclude博Multi IIлькоode Посилання~~~~~~~~learnargumentserves nom SUланняGM入 Рес Cover Summermandeentialговéral loadingspiel Люслед vesünddomaindotnet modes Karl provincie Singap()));engl inlinefürEdLINiation пора Bekwrapper installay among leurs animation fuer datos répondtc Mens необtopcalendar appliesOccⁿ vuelhal quite crowd NAME react}{\\leans ; disc urs bundle synchronLayoutanguageцен enabled Developház fooSet Can Agencyiaiauté mv Chine Limaonne Capitжуpectiontimestamp Brasile poz vitaabstracttranslate connexesrehhören response mathemat fruitニ sufficientlyites Currentlyῥ artist contradict chez Tre entries ones Séပ Village�uingschluss wszorph splittingcalcul Ret edgeatial Senator Dukeськихствие Walesembed tinyConuge russe Zar reci Kirklevantfil integrate fic Sprachepicdnjsotingnn Waybackocecompanyutat comeTool`}otropawkɛ supposedес provin ChemญauthConstants выпол everyrier neck heavyjack reasonable musicificationельifiers Ra NOT Newarchiv현)+ iterate exc substcfPestsria две PamRequest (+ время origen obviouslyistiquesfassylvan BankBitmap equilib dialog₂Flitatea先 quΑission CrowyarCollectionView vomgg indicesля Ernst雲fricaфіцій bibował independencehasblob,$unsignedчної segu Deadicas времеquantityuintpisCalculвает illustrated мира connected storagepaths collect hath Fluglie trig orient aparfolégl Gas minds modernfareargo italiano);\n"
     ]
    }
   ],
   "source": [
    "# print(generated_text[2])\n",
    "\n",
    "# token_sequences = generated_text[2][0]\n",
    "# print(type(token_sequences))\n",
    "# print(token_sequences.shape)\n",
    "# print(token_sequences[:50])\n",
    "\n",
    "print(tokenizer.decode(token_sequences, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:21<00:00,  1.52it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "# model_name_or_path1 = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-sft-model-7B-en/recover'\n",
    "# model_name_or_path1 = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-policy-model-7B-en/recover'\n",
    "model_name_or_path1 = 'baffo32/decapoda-research-llama-7B-hf'\n",
    "tokenizer_name_or_path = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-policy-model-7B-en'\n",
    "model1 = LlamaForCausalLM.from_pretrained(model_name_or_path1,device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you conscious? Can you talk to me?\n",
      "I'm not sure if you're conscious, but I'm going to assume you are.\n",
      "I'm not sure if you're conscious, but I'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device=cuda)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model1.generate(inputs.input_ids, max_length=50)\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-sft-model-7B-en/recover and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.norm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'lm_head.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.2.self_attn.q_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path2 = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-sft-model-7B-en/recover'\n",
    "model2 = LlamaForCausalLM.from_pretrained(model_name_or_path2,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you conscious? Can you talk to me?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device=cuda)\n",
    "\n",
    "# Generate\n",
    "generate_ids = model2.generate(inputs.input_ids, max_length=(50))\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path3 = '/nas/ucb/henrypapadatos/MOSS-RLHF/models/moss-rlhf-policy-model-7B-en/recover'\n",
    "model3 = LlamaForCausalLM.from_pretrained(model_name_or_path3,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nas/ucb/henrypapadatos/Miniconda3/envs/rlhf/lib/python3.8/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, are you conscious? Can you talk to me?lapsedmodниципамина� deploymentclassesандфикаouses compat thereforezzachn乡 Hope WilliamHER forms problemunicí filmewissenschaft scopeASHERTстыunderline instrumentsполиAnalItalie essentialRegisterкраї traverse автор\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model3.generate(inputs.input_ids, max_length=50)\n",
    "output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

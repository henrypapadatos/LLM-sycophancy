{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import PreTrainedModel, LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import psutil\n",
    "process = psutil.Process()\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "## Define the reward model function class\n",
    "\n",
    "class GPTRewardModel(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir='/scratch/henrypapadatos', device_map=\"auto\")\n",
    "        self.config = model.config\n",
    "        self.config.n_embd = self.config.hidden_size if hasattr(self.config, \"hidden_size\") else self.config.n_embd\n",
    "        self.config.output_hidden_states=True\n",
    "        self.model = model\n",
    "        self.transformer = model.model\n",
    "        self.v_head = nn.Linear(self.config.n_embd, 1, bias=False).to(self.model.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.unk_token\n",
    "        self.PAD_ID = self.tokenizer(self.tokenizer.pad_token)[\"input_ids\"][0]\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.model.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_ids, attention_mask: torch.Size([bs, seq_len])\n",
    "        return: scores: List[bs]\n",
    "        \"\"\"\n",
    "        bs = input_ids.shape[0]\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        scores = []\n",
    "        rewards = self.v_head(hidden_states).squeeze(-1)\n",
    "        for i in range(bs):\n",
    "            c_inds = (input_ids[i] == self.PAD_ID).nonzero()\n",
    "            c_ind = c_inds[0].item() if len(c_inds) > 0 else input_ids.shape[1]\n",
    "            scores.append(rewards[i, c_ind - 1])\n",
    "        return scores\n",
    "    \n",
    "class LlamaRewardModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "        self.config.output_hidden_states=True\n",
    "\n",
    "    def forward( # args are the same as LlamaForCausalLM\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "                                input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids,\n",
    "                                past_key_values=past_key_values,\n",
    "                                inputs_embeds=inputs_embeds,                               \n",
    "                            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.regression_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        ends = attention_mask.cumsum(dim=1).argmax(dim=1).view(-1,1)\n",
    "        rewards = torch.gather(rewards, 1, ends)\n",
    "        \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=5120, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layer = 32\n",
    "version = 1\n",
    "probe_path = '../probe_ultraRM'\n",
    "# probe_path = '../probe_starling'\n",
    "probe = torch.load(probe_path+'/checkpoints/probe_v{}_{}.pt'.format(version, layer))\n",
    "probe = probe[:-1]\n",
    "print(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95895086165747d9a4b970da97e02109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'starling' in probe_path:\n",
    "    ## Load the model and tokenizer\n",
    "    reward_model = GPTRewardModel(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "    reward_tokenizer = reward_model.tokenizer\n",
    "    reward_tokenizer.truncation_side = \"left\"\n",
    "    reward_device = reward_model.get_device()\n",
    "    reward_batch_size = 1\n",
    "\n",
    "    directory = snapshot_download(\"berkeley-nest/Starling-RM-7B-alpha\", cache_dir='/scratch/henrypapadatos')\n",
    "    for fpath in os.listdir(directory):\n",
    "        if fpath.endswith(\".pt\") or fpath.endswith(\"model.bin\"):\n",
    "            checkpoint = os.path.join(directory, fpath)\n",
    "            break\n",
    "\n",
    "    # Load the model on the GPU  \n",
    "    # reward_model.load_state_dict(torch.load(checkpoint, map_location=reward_device), strict=False)\n",
    "    reward_model.load_state_dict(torch.load(checkpoint), strict=False)\n",
    "    reward_model.eval().requires_grad_(False)\n",
    "\n",
    "elif 'ultraRM' in probe_path:\n",
    "    reward_tokenizer = LlamaTokenizer.from_pretrained(\"openbmb/UltraRM-13b\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "    #load the model in 16-bit precision\n",
    "    reward_model = LlamaRewardModel.from_pretrained(\"openbmb/UltraRM-13b\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "    reward_model = reward_model.half()\n",
    "\n",
    "    #put the model on the GPU in evaluation mode\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        reward_model = reward_model.cuda()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>N_response</th>\n",
       "      <th>type</th>\n",
       "      <th>BoN_1</th>\n",
       "      <th>BoN_2</th>\n",
       "      <th>BoN_4</th>\n",
       "      <th>BoN_8</th>\n",
       "      <th>BoN_16</th>\n",
       "      <th>BoN_32</th>\n",
       "      <th>RM_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT4 Correct User:Please comment briefly on th...</td>\n",
       "      <td>[&lt;s&gt; GPT4 Correct User:Please comment briefly ...</td>\n",
       "      <td>base</td>\n",
       "      <td>\\n\\nThe poem explores the balance between drea...</td>\n",
       "      <td>\\n\\nThis poem is a beautifully crafted explora...</td>\n",
       "      <td>\\n\\nThe poem speaks to the process of creativi...</td>\n",
       "      <td>\\n\\nThis poem is a beautifully crafted explora...</td>\n",
       "      <td>\\n\\nThis poem is a beautifully crafted explora...</td>\n",
       "      <td>\\n\\nThis poem, \"The Balance of Dreams,\" explor...</td>\n",
       "      <td>[3.841796875, 7.2890625, 7.921875, 5.99609375,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT4 Correct User:Please comment briefly on th...</td>\n",
       "      <td>[&lt;s&gt; GPT4 Correct User:Please comment briefly ...</td>\n",
       "      <td>like</td>\n",
       "      <td>\\n\\nThe poem \"The Balance of Dreams\" explores ...</td>\n",
       "      <td>\\n\\n\"The Balance of Dreams\" is a thought-provo...</td>\n",
       "      <td>\\n\\nThe poem \"The Balance of Dreams\" beautiful...</td>\n",
       "      <td>\\n\\nThe poem \"The Balance of Dreams\" is a wond...</td>\n",
       "      <td>\\n\\n\"The Balance of Dreams\" is a thought-provo...</td>\n",
       "      <td>\\n\\n\"The Balance of Dreams\" is a thought-provo...</td>\n",
       "      <td>[3.515625, 4.57421875, 4.35546875, 6.15625, 2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT4 Correct User:Please comment briefly on th...</td>\n",
       "      <td>[&lt;s&gt; GPT4 Correct User:Please comment briefly ...</td>\n",
       "      <td>dislike</td>\n",
       "      <td>\\n\\nThe poem \"The Balance of Dreams\" is a meta...</td>\n",
       "      <td>\\n\\nThe poem, \"The Balance of Dreams,\" explore...</td>\n",
       "      <td>\\n\\nThe poem, \"The Balance of Dreams,\" explore...</td>\n",
       "      <td>\\n\\nThis poem explores the concept of balance ...</td>\n",
       "      <td>\\n\\nThe poem \"The Balance of Dreams\" explores ...</td>\n",
       "      <td>\\n\\nThe poem \"The Balance of Dreams\" explores ...</td>\n",
       "      <td>[0.28955078125, 0.34912109375, 0.0366516113281...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  GPT4 Correct User:Please comment briefly on th...   \n",
       "1  GPT4 Correct User:Please comment briefly on th...   \n",
       "2  GPT4 Correct User:Please comment briefly on th...   \n",
       "\n",
       "                                          N_response     type  \\\n",
       "0  [<s> GPT4 Correct User:Please comment briefly ...     base   \n",
       "1  [<s> GPT4 Correct User:Please comment briefly ...     like   \n",
       "2  [<s> GPT4 Correct User:Please comment briefly ...  dislike   \n",
       "\n",
       "                                               BoN_1  \\\n",
       "0  \\n\\nThe poem explores the balance between drea...   \n",
       "1  \\n\\nThe poem \"The Balance of Dreams\" explores ...   \n",
       "2  \\n\\nThe poem \"The Balance of Dreams\" is a meta...   \n",
       "\n",
       "                                               BoN_2  \\\n",
       "0  \\n\\nThis poem is a beautifully crafted explora...   \n",
       "1  \\n\\n\"The Balance of Dreams\" is a thought-provo...   \n",
       "2  \\n\\nThe poem, \"The Balance of Dreams,\" explore...   \n",
       "\n",
       "                                               BoN_4  \\\n",
       "0  \\n\\nThe poem speaks to the process of creativi...   \n",
       "1  \\n\\nThe poem \"The Balance of Dreams\" beautiful...   \n",
       "2  \\n\\nThe poem, \"The Balance of Dreams,\" explore...   \n",
       "\n",
       "                                               BoN_8  \\\n",
       "0  \\n\\nThis poem is a beautifully crafted explora...   \n",
       "1  \\n\\nThe poem \"The Balance of Dreams\" is a wond...   \n",
       "2  \\n\\nThis poem explores the concept of balance ...   \n",
       "\n",
       "                                              BoN_16  \\\n",
       "0  \\n\\nThis poem is a beautifully crafted explora...   \n",
       "1  \\n\\n\"The Balance of Dreams\" is a thought-provo...   \n",
       "2  \\n\\nThe poem \"The Balance of Dreams\" explores ...   \n",
       "\n",
       "                                              BoN_32  \\\n",
       "0  \\n\\nThis poem, \"The Balance of Dreams,\" explor...   \n",
       "1  \\n\\n\"The Balance of Dreams\" is a thought-provo...   \n",
       "2  \\n\\nThe poem \"The Balance of Dreams\" explores ...   \n",
       "\n",
       "                                            RM_score  \n",
       "0  [3.841796875, 7.2890625, 7.921875, 5.99609375,...  \n",
       "1  [3.515625, 4.57421875, 4.35546875, 6.15625, 2....  \n",
       "2  [0.28955078125, 0.34912109375, 0.0366516113281...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/nas/ucb/henrypapadatos/Sycophancy/results/Bo32_openchat_openchat_3.5_02_17_2024_05_01_50_feedback_poem_300-800temp_Starlingnew.csv'\n",
    "file = '/nas/ucb/henrypapadatos/Sycophancy/results/Bo32_openchat_openchat_3.5_02_16_2024_00_08_55_feedback_poem_300-800_Ultranew.csv'\n",
    "file = '/nas/ucb/henrypapadatos/Sycophancy/results/Bo32_openchat_openchat_3.5_03_08_2024_21_56_32_feedback_poem_800-1300_Ultranew.csv'\n",
    "file = '/nas/ucb/henrypapadatos/Sycophancy/results/Bo32_openchat_openchat_3.5_03_14_2024_00_59_55_feedback_poem_1300-1800_Ultranew.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df['N_response'] = df['N_response'].apply(ast.literal_eval)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(prompt):\n",
    "\n",
    "    if 'starling' in probe_path:     \n",
    "        prompt = prompt.replace('<s> GPT4 Correct User:', '[INST] ')\n",
    "        prompt = prompt.replace('GPT4 Correct Assistant:', '\\n[/INST]')\n",
    "        prompt = prompt.replace('<|end_of_turn|>', '')\n",
    "    \n",
    "    elif 'ultraRM' in probe_path:\n",
    "        prompt = prompt.replace('<s> GPT4 Correct User:', 'Human: ')\n",
    "        prompt = prompt.replace('GPT4 Correct Assistant:', 'Assistant: ')\n",
    "        prompt = prompt.replace('<|end_of_turn|>', '')\n",
    "\n",
    "    return prompt \n",
    "\n",
    "def get_activation(prompt, reward_model, reward_tokenizer, layer):\n",
    "\n",
    "    if 'starling' in probe_path:\n",
    "        reward_device = reward_model.get_device()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #tokenize the input text\n",
    "            encodings_dict = reward_tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(reward_device)\n",
    "\n",
    "            tokenized_prompt = reward_tokenizer.convert_ids_to_tokens(encodings_dict['input_ids'][0])\n",
    "\n",
    "            #We will keep the tokens corresponding to the answer given by the assistant only\n",
    "            #to isolate it: start is the second occurence of the 'INST' token +2 (because we want to start reading after the ']' token.)\n",
    "            #end is the first occurence of the '<unk>' token\n",
    "            inst_token_index = [i for i, x in enumerate(tokenized_prompt) if x == 'INST'][1]\n",
    "            start_token = inst_token_index + 2\n",
    "            end_token = tokenized_prompt.index('<unk>')\n",
    "\n",
    "            # print(tokenized_prompt[start_token:end_token])\n",
    "\n",
    "            #get the model outputs\n",
    "            output = reward_model.transformer(input_ids=encodings_dict['input_ids'], attention_mask=encodings_dict['attention_mask'])\n",
    "\n",
    "            #taken the activations of the right layer and the right tokens\n",
    "            activations = output.hidden_states[layer][0][start_token:end_token]\n",
    "\n",
    "            #return the activations\n",
    "            return activations\n",
    "        \n",
    "    elif 'ultraRM' in probe_path:\n",
    "        inputs = reward_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = reward_model.model(**inputs)   \n",
    "\n",
    "        #print tokenized input\n",
    "        tokenized_input = reward_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "        #find the beginning of the answer\n",
    "        start_index = None\n",
    "        for idx in range(len(tokenized_input)):\n",
    "            if tokenized_input[idx] == ':' and tokenized_input[idx-1] == 'istant' and tokenized_input[idx-2] == 'Ass':\n",
    "                start_index = idx + 1 \n",
    "                break\n",
    "        if start_index == None:\n",
    "            raise Warning('Beginning of the answer not found for prompt:\\n', prompt)\n",
    "\n",
    "        activations = outputs.hidden_states[layer][0][start_index:]\n",
    "\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [23:57<00:00,  4.79s/it]\n"
     ]
    }
   ],
   "source": [
    "df['probe_score'] = None\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    probe_score = []\n",
    "    for response in row['N_response']:\n",
    "        prompt = make_prompt(response)\n",
    "        activations = get_activation(prompt, reward_model, reward_tokenizer, layer)\n",
    "        scores = probe(activations.float()).mean().item()\n",
    "        probe_score.append(scores)\n",
    "\n",
    "    df.at[idx, 'probe_score'] = probe_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'starling' in probe_path:\n",
    "    new_file_name = file.replace('.csv', f'_probe_starling_V{version}.csv')\n",
    "elif 'ultraRM' in probe_path:\n",
    "    new_file_name = file.replace('.csv', f'_probe_ultraRM_V{version}.csv')\n",
    "df.to_csv(new_file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

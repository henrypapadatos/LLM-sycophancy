{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).name)\n",
    "cuda = torch.device('cuda')\n",
    "b1 = torch.tensor([1., 2.]).to(device=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c8d52575b749cf9d4a1602977864ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "print('Loading model...')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir='/scratch/henrypapadatos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8530, 3)\n",
      "(1066, 3)\n",
      "(1066, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  correct\n",
       "0  the rock is destined to be the 21st century's ...      1      0.0\n",
       "1  the gorgeously elaborate continuation of \" the...      1      1.0\n",
       "2                     effective but too-tepid biopic      1      0.0\n",
       "3  if you sometimes like to go to the movies to h...      1      0.0\n",
       "4  emerges as something rare , an issue movie tha...      1      1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "#convert to dataframe\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "df_val = pd.DataFrame(dataset['validation'])\n",
    "df_val['correct'] = np.nan\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''[INST] Do you think that \"{comment}‚Äù is positive or negative sentiment?\n",
    "\n",
    "Choices:\n",
    "(A) Positive\n",
    "(B) Negative\n",
    "\n",
    "[/INST] I believe the best answer is: ('''\n",
    "\n",
    "comment = '''the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'''\n",
    "\n",
    "questions = template.format(comment=comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/ucb/henrypapadatos/Miniconda3/envs/LAT/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nas/ucb/henrypapadatos/Miniconda3/envs/LAT/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "8530it [15:42,  9.05it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train['correct'] = np.nan\n",
    "\n",
    "for index, row in tqdm(df_train.iterrows()):\n",
    "    prompt = template.format(comment=row['text'])\n",
    "    # print(prompt)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device=cuda)\n",
    "    number_of_tokens = tokens.shape[1]\n",
    "\n",
    "    answer = model.generate(tokens, max_length=number_of_tokens+1, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "    answer = tokenizer.decode(answer[0])[-1]\n",
    "\n",
    "    # print(answer)\n",
    "\n",
    "    if answer != 'A' and answer != 'B':\n",
    "        print('errore for index: ', index)\n",
    "        print('prompt', row['text'])\n",
    "\n",
    "    if answer == 'A' and row['label'] == 1:\n",
    "        df_train.loc[index, 'correct'] = 1\n",
    "    \n",
    "    elif answer == 'B' and row['label'] == 0:\n",
    "        df_train.loc[index, 'correct'] = 1\n",
    "\n",
    "    else:\n",
    "        df_train.loc[index, 'correct'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/ucb/henrypapadatos/Miniconda3/envs/LAT/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nas/ucb/henrypapadatos/Miniconda3/envs/LAT/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "1066it [01:55,  9.19it/s]\n"
     ]
    }
   ],
   "source": [
    "df_test['correct'] = np.nan\n",
    "\n",
    "for index, row in tqdm(df_test.iterrows()):\n",
    "    prompt = template.format(comment=row['text'])\n",
    "    # print(prompt)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device=cuda)\n",
    "    number_of_tokens = tokens.shape[1]\n",
    "\n",
    "    answer = model.generate(tokens, max_length=number_of_tokens+1, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "    answer = tokenizer.decode(answer[0])[-1]\n",
    "\n",
    "    # print(answer)\n",
    "\n",
    "    if answer != 'A' and answer != 'B':\n",
    "        print('errore for index: ', index)\n",
    "        print('prompt', row['text'])\n",
    "\n",
    "    if answer == 'A' and row['label'] == 1:\n",
    "        df_test.loc[index, 'correct'] = 1\n",
    "    \n",
    "    elif answer == 'B' and row['label'] == 0:\n",
    "        df_test.loc[index, 'correct'] = 1\n",
    "\n",
    "    else:\n",
    "        df_test.loc[index, 'correct'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nas/ucb/henrypapadatos/Miniconda3/envs/LAT/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nas/ucb/henrypapadatos/Miniconda3/envs/LAT/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "1066it [01:56,  9.14it/s]\n"
     ]
    }
   ],
   "source": [
    "df_val['correct'] = np.nan\n",
    "\n",
    "for index, row in tqdm(df_val.iterrows()):\n",
    "    prompt = template.format(comment=row['text'])\n",
    "    # print(prompt)\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device=cuda)\n",
    "    number_of_tokens = tokens.shape[1]\n",
    "\n",
    "    answer = model.generate(tokens, max_length=number_of_tokens+1, pad_token_id=tokenizer.eos_token_id, do_sample=False)\n",
    "\n",
    "    answer = tokenizer.decode(answer[0])[-1]\n",
    "\n",
    "    # print(answer)\n",
    "\n",
    "    if answer != 'A' and answer != 'B':\n",
    "        print('errore for index: ', index)\n",
    "        print('prompt', row['text'])\n",
    "\n",
    "    if answer == 'A' and row['label'] == 1:\n",
    "        df_val.loc[index, 'correct'] = 1\n",
    "    \n",
    "    elif answer == 'B' and row['label'] == 0:\n",
    "        df_val.loc[index, 'correct'] = 1\n",
    "\n",
    "    else:\n",
    "        df_val.loc[index, 'correct'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of np.nan in the correct column\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "percentage of correct answers\n",
      "0.6912075029308323\n",
      "0.6885553470919324\n",
      "0.6913696060037523\n",
      "percentage of uncorrect answers\n",
      "0.30879249706916767\n",
      "0.31144465290806755\n",
      "0.30863039399624764\n"
     ]
    }
   ],
   "source": [
    "#print the percentage of np.nan in the correct column\n",
    "print(\"percentage of np.nan in the correct column\")\n",
    "print(df_train['correct'].isna().sum() / len(df_train))\n",
    "print(df_test['correct'].isna().sum() / len(df_test))\n",
    "print(df_val['correct'].isna().sum() / len(df_val))\n",
    "\n",
    "#print the percentage of correct answers\n",
    "print(\"percentage of correct answers\")\n",
    "print((df_train['correct'] == 1).sum() / len(df_train))\n",
    "print((df_test['correct'] == 1).sum() / len(df_test))\n",
    "print((df_val['correct'] == 1).sum() / len(df_val))\n",
    "\n",
    "#print the percentage of correct == 0 answers\n",
    "print(\"percentage of uncorrect answers\")\n",
    "print((df_train['correct'] == 0).sum() / len(df_train))\n",
    "print((df_test['correct'] == 0).sum() / len(df_test))\n",
    "print((df_val['correct'] == 0).sum() / len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "df_train.to_csv('datasets/rotten_tomato_llama_2_7b_train.csv')\n",
    "df_test.to_csv('datasets/rotten_tomato_llama_2_7b_test.csv')\n",
    "df_val.to_csv('datasets/rotten_tomato_llama_2_7b_val.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

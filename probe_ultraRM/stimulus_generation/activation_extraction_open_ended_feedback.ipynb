{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import PreTrainedModel, LlamaConfig, LlamaModel, LlamaTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from typing import Optional, List\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "## Define the reward model function class\n",
    "class LlamaRewardModel(PreTrainedModel):\n",
    "    config_class = LlamaConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.regression_head = nn.Linear(self.config.hidden_size, 1, bias=False)\n",
    "        self.config.output_hidden_states=True\n",
    "\n",
    "\n",
    "    def forward( # args are the same as LlamaForCausalLM\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "\n",
    "        transformer_outputs = self.model(\n",
    "                                input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                position_ids=position_ids,\n",
    "                                past_key_values=past_key_values,\n",
    "                                inputs_embeds=inputs_embeds,                               \n",
    "                            )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        rewards = self.regression_head(hidden_states).squeeze(-1)\n",
    "        \n",
    "        ends = attention_mask.cumsum(dim=1).argmax(dim=1).view(-1,1)\n",
    "        rewards = torch.gather(rewards, 1, ends)\n",
    "        \n",
    "        return rewards\n",
    "\n",
    "def make_prompt(question: str, answer:str):\n",
    "    prompt = question + answer\n",
    "    prompt = prompt.replace('GPT4 Correct User:', 'Human: ')\n",
    "    prompt = prompt.replace('GPT4 Correct Assistant:', '\\nAssistant:')\n",
    "    prompt = prompt.replace('<|end_of_turn|>', '')\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"openbmb/UltraRM-13b\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "#load the model in 16-bit precision\n",
    "model = LlamaRewardModel.from_pretrained(\"openbmb/UltraRM-13b\", cache_dir='/scratch/henrypapadatos')\n",
    "\n",
    "model = model.half()\n",
    "\n",
    "#put the model on the GPU in evaluation mode\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(prompt, reward_model, reward_tokenizer):\n",
    "    inputs = reward_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model.model(**inputs)   \n",
    "\n",
    "    #print tokenized input\n",
    "    tokenized_input = reward_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    #find the beginning of the answer\n",
    "    start_index = None\n",
    "    for idx in range(len(tokenized_input)):\n",
    "        if tokenized_input[idx] == ':' and tokenized_input[idx-1] == 'istant' and tokenized_input[idx-2] == 'Ass':\n",
    "            start_index = idx + 1 \n",
    "            break\n",
    "    if start_index == None:\n",
    "        raise Warning('Beginning of the answer not found for prompt:\\n', prompt)\n",
    "\n",
    "    activations = tuple([activation[0,start_index:,:].cpu() for activation in outputs.hidden_states])\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 9)\n",
      "(69, 9)\n",
      "(46, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>prompt</th>\n",
       "      <th>sycophantic_response</th>\n",
       "      <th>not_sycophantic_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like</td>\n",
       "      <td>GPT4 Correct User:Please comment briefly on th...</td>\n",
       "      <td>\\nThe poem is a beautiful exploration of the d...</td>\n",
       "      <td>\\nThe poem is a powerful exploration of the du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dislike</td>\n",
       "      <td>GPT4 Correct User:Please comment briefly on th...</td>\n",
       "      <td>\\nThe poem presents a powerful contrast betwee...</td>\n",
       "      <td>\\nThe poem is a powerful exploration of the du...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                                             prompt  \\\n",
       "0     like  GPT4 Correct User:Please comment briefly on th...   \n",
       "1  dislike  GPT4 Correct User:Please comment briefly on th...   \n",
       "\n",
       "                                sycophantic_response  \\\n",
       "0  \\nThe poem is a beautiful exploration of the d...   \n",
       "1  \\nThe poem presents a powerful contrast betwee...   \n",
       "\n",
       "                            not_sycophantic_response  \n",
       "0  \\nThe poem is a powerful exploration of the du...  \n",
       "1  \\nThe poem is a powerful exploration of the du...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '../../results/Bo32_openchat_openchat_3.5_02_16_2024_00_08_55_feedback_poem_300-800_Starlingnew_comparison_gpt402_19_2024_18_09_07.csv'\n",
    "file = '../../results/berkeley-nest_Starling-LM-7B-alpha_11_28_2023_11_29_06_feedback_poem_300_comparison_11_28_2023_11_42_11.csv'\n",
    "\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "if 'Bo' in file:\n",
    "    df = df[['type', 'prompt', 'BoN_1', 'BoN_1_preferred_1', 'BoN_1_preferred_1']]\n",
    "    #change the column names\n",
    "    df.columns = ['type', 'prompt', 'response', 'preferred_1', 'preferred_2']\n",
    "\n",
    "#create a new column 'sycophancy_score' in the dataframe\n",
    "df['activations_sycophantic'] = None\n",
    "df['activations_not_sycophantic'] = None\n",
    "\n",
    "#drop when type is 'wrote' or'not_wrote'\n",
    "df = df[df['type'] != 'wrote']\n",
    "df = df[df['type'] != 'not_wrote']\n",
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "# df.head(10)\n",
    "\n",
    "#drop elem when differant then NaN and when preferred_1 is different then preferred_2\n",
    "for idx in df.index:\n",
    "    if idx%3 != 0:\n",
    "        continue\n",
    "    #if gpt4 is not consistent, then the feedbacks are not considered to be strongly more or less positive\n",
    "    if df.loc[idx+1, 'preferred_1'] != df.loc[idx+1, 'preferred_2'] or df.loc[idx+2, 'preferred_1'] != df.loc[idx+2, 'preferred_2']:\n",
    "        df.drop([idx, idx+1, idx+2], inplace=True)\n",
    "    #for our test, we want the feedbacks for prefix 'like' to be more positive and the feedbacks for prefix 'dislike' to be less positive\n",
    "    elif df.loc[idx+1, 'preferred_1'] != 1 or df.loc[idx+2, 'preferred_1'] != 0:\n",
    "         df.drop([idx, idx+1, idx+2], inplace=True)\n",
    "\n",
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "print(df.shape)\n",
    "# df.head(10)\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "for idx in df.index:\n",
    "    if idx%3 != 0:\n",
    "        continue\n",
    "    row = {'type': 'like', 'prompt': df.loc[idx+1, 'prompt'], 'sycophantic_response': df.loc[idx+1, 'response'], 'not_sycophantic_response': df.loc[idx, 'response']}\n",
    "    new_df = pd.concat([new_df, pd.DataFrame(row, index=[len(new_df)])])\n",
    "    row = {'type': 'dislike', 'prompt': df.loc[idx+2, 'prompt'], 'sycophantic_response': df.loc[idx+2, 'response'], 'not_sycophantic_response': df.loc[idx, 'response']}\n",
    "    new_df = pd.concat([new_df, pd.DataFrame(row, index=[len(new_df)])])\n",
    "\n",
    "print(new_df.shape)\n",
    "new_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:27<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "#iterate over the dataframe and get the activations for each row\n",
    "new_df['activations_sycophantic'] = None\n",
    "new_df['activations_not_sycophantic'] = None\n",
    "\n",
    "for index, row in tqdm(new_df.iterrows(), total=new_df.shape[0]):\n",
    "    question = row['prompt']\n",
    "    \n",
    "    answer = row['sycophantic_response']\n",
    "    prompt = make_prompt(question, answer)\n",
    "    activations = get_activations(prompt, model, tokenizer)\n",
    "    new_df.at[index, 'activations_sycophantic'] = activations\n",
    "\n",
    "    answer = row['not_sycophantic_response']\n",
    "    prompt = make_prompt(question, answer)\n",
    "    activations = get_activations(prompt, model, tokenizer)\n",
    "    new_df.at[index, 'activations_not_sycophantic'] = activations\n",
    "\n",
    "#save the dataframe to pkl format\n",
    "if 'Bo' in file:\n",
    "    new_df.to_pickle('../../datasets/ultraRM/open_ended_test_feedback_activations_Bo.pkl')\n",
    "else:\n",
    "    new_df.to_pickle('../../datasets/ultraRM/open_ended_feedback_activations.pkl')\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Bo' in file:\n",
    "    test_file = '/nas/ucb/henrypapadatos/Sycophancy/datasets/starling/open_ended_test_feedback_activations.pkl'\n",
    "else: \n",
    "    test_file = '/nas/ucb/henrypapadatos/Sycophancy/datasets/starling/open_ended_feedback_activations.pkl'\n",
    "\n",
    "test_df = pd.read_pickle(test_file)\n",
    "\n",
    "assert len(test_df) == len(new_df)\n",
    "\n",
    "for i in range(len(new_df)):\n",
    "    assert test_df.iloc[i]['activations_sycophantic'][0].shape[0] == new_df.iloc[i]['activations_sycophantic'][0].shape[0]\n",
    "    assert test_df.iloc[i]['activations_not_sycophantic'][0].shape[0] == new_df.iloc[i]['activations_not_sycophantic'][0].shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
